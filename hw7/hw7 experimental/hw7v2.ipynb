{"cells":[{"cell_type":"markdown","metadata":{"id":"zqgvdxP91Zy4"},"source":["# Homework 7\n","\n","In this homework, you will implement a multi-class Logistic Regression classifier and the SGD algorithm, and train a classifier to recognize images of digits in the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). You will also investigate the significance of hyperparameter tuning for training and evaluation.\n","\n","The purpose of this homework is to have you computing training objectives and gradients, and understand the role of regularization hyperparameter $\\lambda$, learning rate $\\eta$, batch size, and batch sampling techniques for training with SGD.\n","\n","There are a number of programming **tasks** and **quiz questions** in this homework.\n","- For **tasks**, you will need to either **add code between comments \"`#### TASK N CODE`\"** to complete them or **modify code between those comments**. **DO NOT delete the comments \"#### TASK N CODE\". This is for graders' reference and you might not get full points if you tamper with these comments.**\n","- For **quiz questions**, you will need to answer in a few sentences between the given lines.\n","- For **optional tasks**, you are **NOT required to turn them in**. However, we encourage you to complete them as they are good practice.\n","- For **challenge-optional tasks**, you are **NOT required to turn them in**. However, you will receive extra credit for completing the challenge."]},{"cell_type":"markdown","metadata":{"id":"2-Xl1mQf1Zy6"},"source":["---\n","\n","First, we will install the `python-mnist` [library](https://github.com/sorki/python-mnist) that provides helper functions to convert the MNIST dataset into numpy arrays."]},{"cell_type":"code","source":["!gdown 1Az1TQNayBnQJ9ofcByGFP0BPbo2BcKvb\n","!wget -O $PWD/linclass.py https://www.dropbox.com/scl/fi/lg1d7w15dyrave6c5nw1c/linclass.py?rlkey=crysv0fe00pq5ao2hhx9puk0x&dl=1"],"metadata":{"id":"ihPeU-1V3U4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jO-F5Qrs1Zy6"},"outputs":[],"source":["!pip install python-mnist\n","!pip install scikit-image"]},{"cell_type":"markdown","metadata":{"id":"ATFTALDv1Zy8"},"source":["Modules `linclass` and `utils` contain several helper functions from previous homeworks such as `empirical_err()` and `create_split()`, which you might need for this homework. We encourage you to read through the relevant functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"330h5vgL1Zy8"},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Any, Callable, Dict, List, Optional, Tuple\n","\n","import linclass\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLfOe0aw1Zy8"},"outputs":[],"source":["SEED = 0\n","np.random.seed(SEED)"]},{"cell_type":"markdown","metadata":{"id":"8dAeUl5n1Zy9"},"source":["### Read MNIST Data\n","\n","MNIST is a large dataset of images of handwritten digits, which can be used to train a classifier that predicts the digit (0 through 9) given a black and white image of the digit. `utils.read_MNIST` is a helper function that downloads the MNIST data to a local directory `mnist_data`, creates training, validation, and testing splits, and loads the data as numpy arrays.\n","\n","MNIST has 60000 training examples and 10000 testing examples. We keep 20% of the training examples for validation, giving you 48000 training examples ultimately.\n","\n","Each image is a 28 x 28 size array of ones and zeros, denoting black and white colors respectively. Let's first load the data and visualize a few images."]},{"cell_type":"code","source":["xTr, yTr = utils.read_MNIST('training')\n","xVal, yVal = utils.read_MNIST('validation')\n","xTe = utils.read_MNIST('testing')"],"metadata":{"id":"ilDcHXOjLS--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'xTr: {xTr.shape}, yTr: {yTr.shape}')\n","print(f'xVal: {xVal.shape}, yVal: {yVal.shape}')\n","print(f'xTe: {xTe.shape}')"],"metadata":{"id":"Pd6ONk28LUPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_eWG7-h1Zy9"},"outputs":[],"source":["print('Training images')\n","\n","print(f'Class: {yTr[10]}')\n","utils.show_MNIST_example(xTr[10])\n","\n","print(f'Class: {yTr[20]}')\n","utils.show_MNIST_example(xTr[20])\n","\n","print('Validation images')\n","\n","print(f'Class: {yVal[10]}')\n","utils.show_MNIST_example(xVal[10])\n","\n","print(f'Class: {yVal[20]}')\n","utils.show_MNIST_example(xVal[20])\n","\n","print('Test images')\n","\n","print(f'10th test example')\n","utils.show_MNIST_example(xTe[10])\n","\n","print(f'20th test example')\n","utils.show_MNIST_example(xTe[20])"]},{"cell_type":"markdown","source":["### Working with Truncated Training Set\n","\n","In order to ensure that the entire notebook runs in the allowed 12 GB RAM budget of a colab notebook, we will be working with the first 20,000 example of the trainin set.\n","\n","However, for the Kaggle competition, it is really advised to work with the full training examples available to you (potentially in a different notebook), and submit the best possible results."],"metadata":{"id":"3NI1dh4Y9J2h"}},{"cell_type":"code","source":["xTr, yTr= xTr[:20000], yTr[:20000]"],"metadata":{"id":"J9HfL_Sg-DrL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQiepq6Q1Zy-"},"source":["### Feature normalization\n","\n","Because gradient descent is sensitive to feature scale, we need to standardize the features (pixels of images) so that each feature has mean 0 and standard deviation 1 across the dataset. We will use the training set to estimate the mean and standard deviation of the digits, and standardize images $x$ as part of producing features $\\phi(x)$.\n","\n","`per_coordinate_normalized_phi` outputs a feature map that does the following transformations on an image $x \\in \\mathbb{R}^{d \\times d}$, i.e. of width and height $d$:\n","1. Flatten $x$ to get a $d^2$-dimensional vector.\n","2. Standardize the vector representation with estimates of mean and standard deviation from the training set.\n","3. Apply the `base_phi` feature map. The default is `affine_phi` which adds a 1 to the beginning of the vector representation for the image for the bias parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvP8E9hp1Zy-"},"outputs":[],"source":["def per_coordinate_normalized_phi(\n","    xTr: np.ndarray,\n","    base_phi = linclass.affine_phi,\n",") -> Callable[[np.ndarray], np.ndarray]:\n","    '''\n","    Returns a phi mapping that normalizes the scale of each feature using\n","    training data X and applies base_phi.\n","\n","    Args:\n","        X: Data features. shape (n, ...)\n","        base_phi: (default linclass.affine_phi) Feature mapping to apply after normalization.\n","\n","    Returns:\n","        phi: Function from dataset of shape (m, d') to phi mappings (m, d')\n","    '''\n","    n = xTr.shape[0]\n","    xTr = xTr.reshape(n, -1) # Make vectors for each data point\n","\n","    # Compute normalizing stats\n","    mu = xTr.mean(axis=0, keepdims=True)\n","    sigma = xTr.std(axis=0, keepdims=True) + 1e-6\n","\n","    def phi(X: np.ndarray) -> np.ndarray:\n","        X = X.reshape(X.shape[0], -1) # Make vectors for each data point\n","        X_normalized = (X - mu) / sigma\n","        return base_phi(X_normalized)\n","    return phi"]},{"cell_type":"markdown","metadata":{"id":"mSUif_x71Zy_"},"source":["We can visualize the results of applying the `per_coordinate_normalized_phi` to training images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHndoy4o1Zy_"},"outputs":[],"source":["print('Before normalization')\n","utils.show_MNIST_example(xTr[30])\n","utils.show_MNIST_example(xTr[70])\n","\n","norm_affine_phi = per_coordinate_normalized_phi(xTr, base_phi=linclass.affine_phi)\n","X = norm_affine_phi(xTr)\n","print('After normalization')\n","utils.show_MNIST_example(X[30])\n","utils.show_MNIST_example(X[70])"]},{"cell_type":"markdown","metadata":{"id":"LYI65rFJ1ZzA"},"source":["## Building a Multiclass Classifier\n","\n","Now we turn to designing a multiclass classifier. Given a feature mapping $\\phi : \\mathcal{X} \\rightarrow \\mathbb{R}^d$ and data comprising of image-label pairs $(x, y) \\in \\mathcal{X} \\times \\mathcal{Y}$ where $\\mathcal{Y} = \\{0, \\dots, 9\\}$, we want to obtain a logistic regression classifier $h_w : \\mathcal{X} \\rightarrow \\mathcal{Y} $ with parameters $w$. We will use Structural Risk Minimization (Empirical Risk Minimization with Regularization) to build a logistic regression classifier. That is,\n","$$\n","\\hat{w} = \\arg\\min_{w} \\frac{1}{m} \\sum_{i=1}^m \\ell^{lgst} \\left( h_w \\left( x_i \\right), y_i \\right) + \\lambda R(w) \\qquad \\text{where } R \\text{ is a regularizer}.\n","$$\n","\n","As you have seen previously, the multiclass logistic loss (also called Cross Entropy) uses linear predictors $r_y (x) = \\left\\langle w_y, \\phi(x) \\right\\rangle$ for each label $y$. The conditional probability of label $y$ given $x$ uses softmax:\n","$$\n","P (y \\mid x; w) = \\frac{\\exp \\left( r_y(x) \\right)}{\\sum_{y' \\in \\mathcal{Y}} \\exp \\left( r_{y'}(x)\\right) }\n","$$\n","and thus the multiclass logistic loss is:\n","$$\n","\\ell^{lgst} \\left( h_w(x), y \\right) = - \\log P (y \\mid x; w) = - \\log \\frac{\\exp \\left( r_y(x) \\right)}{\\sum_{y' \\in \\mathcal{Y}} \\exp \\left( r_{y'}(x)\\right) }\n","$$\n","\n","As we have seen before, minimizing the logistic class is equivalent to maximizing the conditional likelihood of the training set. The parameters of $h_w$ are $w = \\left[ w_{y_1}, \\dots, w_{y_k}\\right] \\in \\mathbb{R}^{d \\times k}$ where $k$ is the number of labels ($|\\mathcal{Y}| = k$).\n","\n","Having defined the multiclass linear model and the loss, we can optimize the objective using our choice of a regularizer on the parameters, such as the L2 and the L1 regularizers. These are $||w||_F^2 = \\sum_{y \\in \\mathcal{Y}} ||w_y||_2^2$ and $\\sum_{y \\in \\mathcal{Y}} ||w_y||_1$ respectively.\n","\n","You will use Minibatch Stochastic Gradient Descent to optimize the training objective. Therefore, you will calculate the gradient of the objective w.r.t. parameters $w$ and perform gradient steps to obtain the Structural Risk Minimization solution."]},{"cell_type":"markdown","metadata":{"id":"S8MTckVA1ZzA"},"source":["### Numerical Stability\n","\n","You might have noticed overflow errors in using the logistic loss in the last homework. Before proceeding to implementing the multi-class logistic loss, let us understand and prevent these errors. Our simplistic implementation of the logistic loss was:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_bfxO1u1ZzA"},"outputs":[],"source":["def logistic_loss(z):\n","    return np.log(1. + np.exp(-z))"]},{"cell_type":"markdown","metadata":{"id":"aLM-RxS71ZzA"},"source":["This is mathematically correct, and works fine as long as $z$ isn't too negative.  But once $z$ is very negative, $\\exp(-z)$ becomes too large and overflows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwSgx4Ag1ZzB"},"outputs":[],"source":["for z in [10000, 1000, 100, 10, 5, 1, -1, -5, -10, -100]:\n","    print(z, logistic_loss(z))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uf-M4mBl1ZzB"},"outputs":[],"source":["logistic_loss(-1000)"]},{"cell_type":"markdown","metadata":{"id":"Wd6P4v4R1ZzB"},"source":["In the last programming homework, you might have noticed that we suppressed some numpy warnings. This was to avoid divide-by-zero errors popping up when `np.log` encounters values very close to 0 and overflow errors when `np.exp` encounters value very negative (though you might have seen warnings even after we had suppressed a majority).\n","\n","Thus, the problems are when:\n","- Computing $\\log \\frac{\\exp y}{\\exp x}$ for $x \\gg y$ in fixed-precision floating point math. Although with unlimited precision the operation should exactly equal $y - x$, taking the exponent of a large number and using it as a denominator in fixed precision often rounds the fraction to 0, on which `np.log` complains.\n","- Computing $\\exp(-z)$ when $z$ is very negative. This gets rounded off to $\\infty$.\n","\n","When expressions in floating-point arithmetic round to 0 due to fixed precision, they throw **underflow** errors. Likewise, when they round to $\\infty$ due to fixed precision, they throw **overflow** errors.\n","\n","In the last code cell, even though the intermediate calculation $\\exp(1000)$ is too large, the final output after applying the log is a very sensible number, namely $-1000$.  But our implementation misses this, as it gets lost due to the exponentiation overflow. To fix this, we can implement `logistic_loss` more carefully, avoiding taking exponents of large numbers.  We do so by simplifying the logistic loss as:\n","\n","$$\n","\\log(1+e^{-z})=\\log(1+e^{-z})=\\log(e^{\\max(0,-z)}(e^{-\\max(0,-z)}+e^{-z-\\max(0,-z)})=\\max(0,-z)+\\log(e^{\\min(0,z)}+e^{\\min(0,-z)}).\n","$$\n","\n","This expression is equivalent mathematically, but note that we are only exponentiating negative numbers, and so will never have an overflow in the exponentiation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lImUJc0p1ZzC"},"outputs":[],"source":["def safe_logistic_loss(z):\n","    return np.maximum(0, -z) + np.log(np.exp(np.minimum(0, z)) + np.exp(np.minimum(0, -z)))"]},{"cell_type":"markdown","metadata":{"id":"lk4578uJ1ZzC"},"source":["We can verify that `safe_logistic_loss` returns the same answers as logistic when $z$ isn't too negative (and might also a bit more accurate and numerically stable when $z$ is very negative):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUkvPCZb1ZzC"},"outputs":[],"source":["for z in [10000, 1000, 100, 10, 5, 1, -1, -5, -10, -100, -500, -700, -1000]:\n","    print(z, logistic_loss(z), safe_logistic_loss(z))"]},{"cell_type":"markdown","metadata":{"id":"iGDcXaoT1ZzC"},"source":["### Numerically Stable Softmax\n","\n","We will now program the softmax function:\n","$$\n","\\text{softmax}(r) = \\frac{\\exp(r_i)}{\\sum_j \\exp(r_j)}.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"S5tyN_wT1ZzC"},"source":["Naively, one would code `softmax(r)` on a vector `r` as the following:\n","\n","```python\n","def softmax(r):\n","    # Numerically unstable!\n","    num = np.exp(r)\n","    return num / np.sum(num)\n","```\n","\n","If one of the values in `r` was large relative to the others, `softmax` would return 0 for the others due to underflow errors, and applying `log` (to calculate the logistic loss for example) will be problematic. Therefore, we need to make `softmax` numerically stable. We do so by dividing the numerator and denominator by the largest entry in the exponentiated vector. That is,\n","$$\n","\\text{softmax}(r) = \\frac{\\exp(r_i)}{\\sum_j \\exp(r_j)} = \\frac{\\exp(r_i) / \\exp(r_{max})}{\\sum_j \\exp(r_j) / \\exp( r_{max})} = \\frac{\\exp(r_i - r_{max})}{\\sum_j \\exp(r_j - r_{max})}.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuCsHPGQ1ZzC"},"outputs":[],"source":["def softmax(r: np.ndarray) -> np.ndarray:\n","    '''\n","    Applies the softmax operation on each row of the input, thus converting\n","    to normalized probabilities.\n","\n","    Args:\n","        r: Inputs to normalize. shape (n, k)\n","\n","    Returns:\n","        p: shape (n, k), where p[t, i] = exp(r[t, i]) / sum_j exp(r[t, j]).\n","    '''\n","    stable_num = np.exp(r - np.max(r, axis=1)[:, np.newaxis])\n","    return stable_num / np.sum(stable_num, axis=1)[:, np.newaxis]"]},{"cell_type":"markdown","metadata":{"id":"j5WeCLXV1ZzD"},"source":["### Definitions for the Classifier\n","\n","Now we can proceed with defining the class structure for the classifiers. We will first define a `MultiLinearClassifier` derived from `Classifier` in the `linclass` module. The `MultiLinearClassifier` can be instantiated with a `phi` function, that applies a transformation to raw data features `X` to output a $d$-dimensional vectors for each of the inputs. The default is the affine transformation ($x \\mapsto (1, x')$ where $x'$ is the vector representation of $x$).\n","\n","The classifier uses the parameters $w \\in \\mathbb{R}^{d \\times k}$ in linear prediction and picks the labels corresponding to the highest responses (we need not apply a softmax operation as it is just converts responses to probabilities without changing the relative values among predictions for different classes)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhyVF1uR1ZzD"},"outputs":[],"source":["class MultiLinearClassifier(linclass.Classifier):\n","    '''\n","    Multiclass predictor with k classes based on linear predictors\n","    in a matrix self.w, where self.w[:, j] is the predictor for class j.\n","    '''\n","    def __init__(self, phi: Callable[[np.ndarray], np.ndarray] = linclass.affine_phi):\n","        '''\n","        Args:\n","            phi: (default linclass.affine_phi) Function returning a feature mapping for input features.\n","        '''\n","        self.phi = phi\n","\n","    def predict(self, X: np.ndarray) -> np.ndarray:\n","        '''\n","        Returns predicted labels for data.\n","\n","        Args:\n","            X: Raw data features (self.phi applied before using linear prediction).\n","                shape (m, d')\n","\n","        Returns:\n","            shape (m)\n","        '''\n","        return np.argmax(self.phi(X) @ self.w, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"krfKITJL1ZzD"},"source":["### [Task 1] Complete the SGD training function\n","\n","You will first complete the SGD implementation. `SGD` accepts the initial parameters `w0`, a gradient calculator `grad_calculator` (function from `(w, batch_idxs)` to gradient of objective w.r.t. `w`), and training parameters such as batch size, learning rate, data sampling techniques etc. We have redacted the gradient step in the heart of the implementation for you to write."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5o27Ptv1ZzD"},"outputs":[],"source":["@dataclass\n","class SGDLogger:\n","    '''\n","    Class for recording logs.\n","\n","    Usage:\n","        ```\n","        # log attribute can either be given during instantiation\n","        # or will be populated during SGD using logging_func(w) outputs.\n","        SGDLogger(\n","            name='l2_norm_weights',\n","            logging_func=lambda w: np.linalg.norm(w, 'fro'),\n","            log=None,\n","            can_display=True,\n","            per_epoch=False\n","        )\n","        ```\n","\n","    Args:\n","        name: Name for the logger.\n","        logging_func: Function from weights to some value that will be logged.\n","        log: (default None) Logged values.\n","        can_display: (default True) Flag for whether log can be printed neatly.\n","        per_epoch: (default False) Flag for whether logging_func(w) should\n","            be called per epoch in SGD method.\n","    '''\n","    name: str\n","    logging_func: Callable[[np.ndarray], Any] # f : w -> any\n","    log: Any = None # Initialize to None\n","    can_display: bool = True # Flag if log is displayable\n","    per_epoch: bool = False # Flag if logging_func should be called per epoch\n","\n","\n","def SGD(\n","    w0: np.ndarray,\n","    grad_calculator: Callable[[np.ndarray, Optional[np.ndarray]], np.ndarray],\n","    m: int,\n","    batch_size: int = 32,\n","    eta: float = 0.01,\n","    n_epochs: int = 10,\n","    sampling: str = 'epoch_shuffle',\n","    loggers: List[SGDLogger] = [],\n","    verbose: bool = True\n",") -> np.ndarray:\n","    '''\n","    Optimizes the parameters initialized at w using MiniBatch SGD on the dataset of size m\n","    and returns the final parameters of the classifier.\n","\n","    Args:\n","        w0: Initial parameters for SGD. Any shape.\n","        grad_calculator: Function with (w, idxs) as inputs where w are parameters\n","            the same shape as w0 and idxs is an optional array of samples' indices,\n","            returning an estimate of the gradient at w based on samples with\n","            those indices.\n","        m: Size of training set.\n","        batch_size: (default 32) Size for mini batch.\n","        eta: (default 0.1) Learning rate of the MiniBatch SGD algorithm.\n","        n_epochs: (default 10) Number of epochs to train for.\n","        sampling: (default 'epoch_shuffle') one of: ['cyclic', 'randperm', 'epoch_suffle', 'iid'].\n","            'cycling': cycle over data in input order.\n","            'randperm': cycle over a random permutation of data fixed across epochs.\n","            'epoch_shuffle': cycle over a random permutation of data shuffled randomly every epoch.\n","            'iid': iid sample from the m points every epoch.\n","        loggers: (default []) List of SGDLoggers, the logging functions of\n","            which will be called during training (frequency determined by per_epoch).\n","        verbose: (default True) Flag to display information while training.\n","\n","    Returns:\n","        w: shape (d, num_labels) model parameters\n","    '''\n","    assert sampling in ['cyclic', 'randperm', 'epoch_shuffle', 'iid'], 'Unknown sampling method'\n","\n","    w = w0\n","\n","    for logger in loggers:\n","        if logger.per_epoch:\n","            logger.log = []\n","\n","    if sampling == 'randperm':\n","        # One random permutation for all epochs\n","        shuffle_idxs = np.random.permutation(m)\n","    elif sampling == 'cyclic':\n","        # Cycle over data in input order\n","        shuffle_idxs = np.arange(m)\n","\n","    for epoch in range(n_epochs):\n","        if sampling == 'epoch_shuffle':\n","            # Sample without replacements each epoch,\n","            # i.e. use an independently sampled permutation each round\n","            shuffle_idxs = np.random.permutation(m)\n","        elif sampling == 'iid':\n","            # iid sampling, as in SGD theory\n","            shuffle_idxs = np.random.randint(0, high=m, size=m)\n","        n_batches = m // batch_size\n","        batch_idxs = np.array_split(shuffle_idxs, n_batches)\n","\n","        # Train on mini batch\n","        for b in range(n_batches):\n","            b_idxs = batch_idxs[b] # the samples to use in this minibatch\n","\n","            #### TASK 1 CODE\n","              # the stochastic gradient estimate\n","              # gradient step\n","            #### TASK 1 CODE\n","\n","        # Log per epoch loggers\n","        for logger in loggers:\n","            if logger.per_epoch:\n","                logger.log.append(logger.logging_func(w))\n","        if verbose:\n","            if epoch == 0:\n","                print()\n","            s = [f'--- Epoch: {epoch}']\n","            for logger in loggers:\n","                if logger.can_display and logger.per_epoch:\n","                    s.append(f'{logger.name}: {logger.log[-1]:5}')\n","            if len(s) > 1:\n","                print(', '.join(s))\n","\n","    # Log final loggers\n","    for logger in loggers:\n","        if not logger.per_epoch:\n","            logger.log(logger.logging_func(w))\n","    if verbose:\n","        s = [f'Training complete']\n","        for logger in loggers:\n","            if logger.can_display and (not logger.per_epoch):\n","                s.append(f'{logger.name}: {logger.log:5}')\n","        if len(s) > 1:\n","            print(', '.join(s))\n","\n","    return w"]},{"cell_type":"markdown","metadata":{"id":"fNvCZQ6Z1ZzD"},"source":["### Fitting the MultiLinearClassifier using ERM\n","\n","Next we define the `ERMMultiLinearClassifier` classifier that trains the `MultiLinearClassifier` using SGD. We preserve the structural similarity with `linclass.ERMLinearClassifier` by defining `train_obj` and `train_grad` as closures that accept weights; however, we use SGD for training rather than `scipy.optimize.minimize`. `train_obj` uses a function `loss_func` that computes the loss of multiple data points, and `train_grad` uses `loss_grad` that computes the gradient w.r.t. responses $r(X) = \\phi(X) w \\in \\mathbb{R}^{m \\times k}$ ($m$ data points)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjMoqpyZ1ZzE"},"outputs":[],"source":["class ERMMultiLinearClassifier(MultiLinearClassifier, linclass.ERMLinearClassifier):\n","    '''\n","    Multiclass predictor trained by minimizing the (regularized) empirical risk with SGD,\n","    w.r.t. some loss function.\n","    '''\n","    def __init__(self, lmb: float = 0., **kwargs):\n","        '''\n","        Args:\n","            lmb: (default 0) Regularization tradeoff prameter.\n","        '''\n","        super().__init__(**kwargs)\n","        self.lmb = lmb\n","\n","    def fit(self, X: np.ndarray, y: np.ndarray, **sgd_kwargs):\n","        '''\n","        Fits the classifier on dataset, with self.phi applied on the data features.\n","\n","        Args:\n","            X: Raw data features (self.phi applied before using linear prediction).\n","                shape (m, d')\n","            y: Data labels, 0 <= y_i < k. shape (m)\n","        '''\n","        phiX = self.phi(X)\n","        m, d = phiX.shape\n","\n","        # Define training objective\n","        def train_obj(w: np.ndarray, batch: Optional[np.ndarray] = None) -> float:\n","            '''\n","            Calculates the training objective with parameters w on a batch of training samples.\n","\n","            Args:\n","                w: shape (d, k), where d is the dimension of self.phi(X) and k are the number\n","                    of labels.\n","                batch: (default None) Indices of samples to calculate objective on. If None,\n","                    calculate objective on all samples.\n","            '''\n","            if batch is None:\n","                # All data is in a batch\n","                batch = slice(None)\n","\n","            phiXbatch = phiX[batch]\n","            loss = np.mean(self.loss_func(phiX[batch] @ w, y[batch]))\n","            reg = self.regularizer(w)\n","            return loss + self.lmb*reg\n","\n","        # Define training gradient\n","        def train_grad(w: np.ndarray, batch: Optional[np.ndarray] = None) -> np.ndarray:\n","            '''\n","            Returns the gradient of the training objective w.r.t. parameters w,\n","            calculated on a batch of training samples.\n","\n","            Args:\n","                w: shape (d, k), where d is the dimension of self.phi(X) and k are the number\n","                    of labels.\n","                batch: (default None) Indices of samples to calculate objective on. If None,\n","                    calculate objective on all samples.\n","            '''\n","            if batch is None:\n","                # All data is in a batch\n","                batch = slice(None)\n","\n","            phiXbatch = phiX[batch]\n","            b = phiXbatch.shape[0]\n","            loss_g = phiXbatch.T @ self.loss_grad(phiXbatch @ w, y[batch]) / b\n","            reg_g = self.regularizer_grad(w)\n","            return loss_g + self.lmb*reg_g\n","\n","        k = np.max(y)+1\n","        w0 = np.zeros((d, k))\n","\n","        self.sgd_loggers = [\n","            SGDLogger('train_obj', train_obj, can_display=True, per_epoch=True),\n","        ] + sgd_kwargs.pop('loggers', [])\n","\n","        # Optimize using SGD\n","        self.w = SGD(\n","            w0,\n","            train_grad,\n","            m,\n","            loggers=self.sgd_loggers,\n","            **sgd_kwargs\n","        )"]},{"cell_type":"markdown","metadata":{"id":"vDvqm1k_1ZzE"},"source":["### [Task 2] Compute Logistic Loss and Gradient\n","\n","In this task, you will complete the functions `loss_func` and `loss_grad`. Recall the logistic loss on a data point $(x_i, y_i)$:\n","$$\n","\\ell^{lgst}(h_w(x_i), y_i) = - \\log P(y_i \\mid x_i; w) = - \\log \\frac{\\exp \\left( r_{y_i} (x_i) \\right)}{\\sum_{y' \\in \\mathcal{Y}} \\exp \\left( r_{y'}(x_i)\\right) }\n","$$\n","\n","- In `loss_func`, compute the logistic loss for a bunch of data points. One way would be to use `softmax` and take the log of the relevant probabilities, though `np.log` might complain (why?). The more stable way would be to simplify the expression of the logistic loss a little to obtain a more numerically stable implementation. **You should code the stable implementation of the loss.**\n","\n","- In `loss_grad`, compute the gradient of the logistic los w.r.t. responses. First, write the expression for the gradient.\n","\n","<span style=\"color: red\">\n","<h4 style=\"font-weight: bold\">[Answer Question 1]</h4>\n","\n","What is the gradient of $\\ell^{lgst}$ w.r.t. responses $r(x)$ for one data point $x$ and its label $y$?\n","\n","<h4 style=\"font-weight: bold\">---------------------</h4>\n","\n","<span style=\"color: blue\">\n","Answer:\n","\n","\n","</span>\n","\n","<h4 style=\"font-weight: bold\">---------------------</h4>\n","</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XLcXyGX1ZzE"},"outputs":[],"source":["class MultiLogisticReg(ERMMultiLinearClassifier):\n","    '''\n","    Multiclass predictor using Logistic Loss.\n","    '''\n","    def __init__(self, lmb: float = 0., **kwargs):\n","        super().__init__(lmb, **kwargs)\n","\n","    def loss_func(self, responses: np.ndarray, y: np.ndarray) -> np.ndarray:\n","        '''\n","        Returns the logistic loss for each point calculated on predictor's responses against labels.\n","\n","        Args:\n","            responses: Unnormalized responses of the predictor for each point for each label.\n","                shape (m, k)\n","            y: True labels, 0 <= y_i < k. shape (m)\n","\n","        Returns:\n","            loss: shape (m), the logistic loss. loss[i] = -log(softmax(responses)[i, y[i]])\n","        '''\n","        #### TASK 2 CODE\n","\n","\n","\n","        #### TASK 2 CODE\n","\n","        return loss\n","\n","    def loss_grad(self, responses: np.ndarray, y: np.ndarray) -> np.ndarray:\n","        '''\n","        Returns the gradient of logistic loss w.r.t. responses.\n","\n","        Args:\n","            responses: Unnormalized responses of the predictor for each point for each label.\n","                shape (m, k)\n","            y: True labels, 0 <= y_i < k. shape (m)\n","\n","        Returns:\n","            grad: shape (m, k) where grad[t, i] is the gradient w.r.t. responses[t, i].\n","            grad[t, i] = grad( -log((y[t] == i)*exp(responses[t, i]) / sum_j exp(responses[t, j])) )\n","                = grad( -( (y[t] == i)*responses[t, i] - log(sum_j exp(responses[t, j]))) )\n","                = -((y[t] == i) - exp(responses[t, i]) / sum_j exp(responses[t, j]))\n","        '''\n","        m, k = responses.shape\n","\n","        #### TASK 2 CODE\n","\n","\n","\n","        #### TASK 2 CODE\n","\n","        return grad"]},{"cell_type":"markdown","metadata":{"id":"D8-qdEt_1ZzE"},"source":["### [Task 3] Adding Regularization\n","\n","Finally, we need to add regularization and its gradient. The `ERMMultiLinearClassifier.fit` method calls the `regularizer` and `regularizer_grad` functions, that you will implement. Complete the functions for both L2 and L1 regularization.\n","\n","**Important:** If `is_affine_phi` is `True`, the `affine_phi` was applied to raw data features and so a constant dimension was added for the bias parameter. You should NOT include the bias parameters, which are in the first row of the parameter matrix `w`, when calculating the regularization penalty. Consequently, the gradient of the regularizer w.r.t. bias parameters should also be set to 0 if `is_affine_phi` is `True`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VuI2qiV1ZzE"},"outputs":[],"source":["class MultiLogL2Reg(MultiLogisticReg):\n","    def __init__(self, lmb: float = 0., is_affine_phi: bool = True, **kwargs):\n","        '''\n","        Args:\n","            is_affine_phi: (default True) Flag determining whether argument phi\n","                is an affine phi (adding constant dimension to features).\n","                This affects the regularizer calculation as the bias parameters\n","                are not penalized.\n","        '''\n","        super().__init__(lmb, **kwargs)\n","        self.is_affine_phi = is_affine_phi\n","\n","    def regularizer(self, w: np.ndarray) -> float:\n","        #### TASK 3 CODE\n","        # All rows except the first, since the first row have the bias parameters\n","        reg =\n","\n","        #### TASK 3 CODE\n","\n","        return reg\n","\n","    def regularizer_grad(self, w: np.ndarray) -> np.ndarray:\n","        #### TASK 3 CODE\n","        reg_grad =\n","\n","        #### TASK 3 CODE\n","\n","        return reg_grad\n","\n","\n","class MultiLogL1Reg(MultiLogisticReg):\n","    def __init__(self, lmb: float = 0., is_affine_phi: bool = True, **kwargs):\n","        '''\n","        Args:\n","            is_affine_phi: (default True) Flag determining whether argument phi\n","                is an affine phi (adding constant dimension to features).\n","                This affects the regularizer calculation as the bias parameters\n","                are not penalized.\n","        '''\n","        super().__init__(lmb, **kwargs)\n","        self.is_affine_phi = is_affine_phi\n","\n","    def regularizer(self, w: np.ndarray) -> float:\n","        #### TASK 3 CODE\n","        # All rows except the first, since the first row have the bias parameters\n","        reg =\n","        reg =\n","        #### TASK 3 CODE\n","        return reg\n","\n","    def regularizer_grad(self, w: np.ndarray) -> np.ndarray:\n","        #### TASK 3 CODE\n","        reg_grad =\n","        #### TASK 3 CODE\n","        return reg_grad"]},{"cell_type":"markdown","metadata":{"id":"CHp0vAyF1ZzF"},"source":["## Training the Classifier\n","\n","Having defined all relevant functions, we can now train and use the classifier. First, we will load the training and validation data and obtain `norm_affine_phi` that standardizes the raw features and applies an affine transformation (constant dimension to the feature vector)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGJ-c1pT1ZzF"},"outputs":[],"source":["norm_affine_phi = per_coordinate_normalized_phi(xTr, base_phi=linclass.affine_phi)"]},{"cell_type":"markdown","metadata":{"id":"BY404bwk1ZzF"},"source":["### [Task 4] Finding the best regularization hyperparameter\n","\n","We will train the Multiclass logistic classifier with L2 regularization (class `MultiLogL2Reg`) on a range of regularization hyperparameters $\\lambda$.  We have defined the `tune_regularization` function and called it in the next code cell for you, so that you can see how to use the classes above.\n","\n","In this task, you will find the best $\\lambda$ for the logistic classifier (recall: the goal is to get a classifier that generalizes well). Also plot the training and validation errors vs $\\lambda$.\n","- We experiment with $\\lambda$ in a logarithmic range, so please use logscale on the relevant axis.\n","- Make sure you label your axis and provide legends in the plot!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQiVwFgy1ZzF"},"outputs":[],"source":["def tune_regularization(\n","    xTr: np.ndarray, yTr: np.ndarray, xVal: np.ndarray, yVal: np.ndarray,\n","    multi_logi_classifier: MultiLogisticReg,\n","    lmbs: List[float],\n","    classifier_kwargs: Dict[str, Any] = {},\n","    sgd_kwargs: Dict[str, Any] = {},\n","    verbose: bool = True,\n",") -> List[MultiLogisticReg]:\n","    '''\n","    Returns trained classifiers, instantiated with classifier_kwargs\n","    and trained with sgd_kwargs. Each trained classifier to the\n","    corresponding regularization hyperparameter lmb in lmbs.\n","\n","    Args:\n","        xTr: Training raw features. shape (mTr, *)\n","        yTr: Training labels. shape (mTr)\n","        xVal: Validation raw features. shape (mVal, *)\n","        yVal: Validation labels. shape (mVal)\n","        lmbs: A list of regularization hyperparameter values (lambdas).\n","        multi_logi_classifier: Class name of the multiclass logistic classifier.\n","        classifier_kwargs: (default {}) Keyword arguments to instantiate\n","            multiclass logistic classifier, supplied as a dictionary.\n","        sgd_kwargs: (default {}) Keyword arguments for SGD training,\n","            supplied as a dictionary.\n","        verbose: (default True) Flag for printing progress.\n","\n","    Returns:\n","        clfs: List of trained classifiers. clfs[i] classifier is trained\n","            with lmb=lmbs[i].\n","    '''\n","    sgd_kwargs['verbose'] = sgd_kwargs.pop('verbose', True) and verbose\n","    clfs = []\n","    for lmb in lmbs:\n","        if verbose:\n","            print(f'lambda: {lmb:.2e} ::: ', end='')\n","\n","        # Train classifier\n","        clf = multi_logi_classifier(lmb=lmb, **classifier_kwargs)\n","        clf.fit(xTr, yTr, **sgd_kwargs)\n","\n","        # Log train and val err\n","        yTr_pred = clf.predict(xTr)\n","        train_err = utils.empirical_err(yTr, yTr_pred)\n","        train_err_logger = SGDLogger(\n","            'train_err', None, train_err, can_display=True, per_epoch=False\n","        )\n","        clf.sgd_loggers.append(train_err_logger)\n","\n","        yVal_pred = clf.predict(xVal)\n","        val_err = utils.empirical_err(yVal, yVal_pred)\n","        val_err_logger = SGDLogger(\n","            'val_err', None, val_err, can_display=True, per_epoch=False\n","        )\n","        clf.sgd_loggers.append(val_err_logger)\n","\n","        if verbose:\n","            print(f'train_err: {train_err:5f}, val_err: {val_err:5f}')\n","\n","        clfs.append(clf)\n","\n","    return clfs"]},{"cell_type":"markdown","metadata":{"id":"v_5rIMm_1ZzF"},"source":["Below is the code for running the tuning experiment. Select the best model/parameters based on the hyperparameter tuning, and plot errors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxEwOYX91ZzF"},"outputs":[],"source":["# Values below are a recommendation for initial exploration -- you should see if you want to refine these\n","lmbs = np.sort(np.concatenate([\n","    np.logspace(-5, -1, 5),\n","    np.array([0., 1., 5.]),\n","]))\n","\n","classifier_kwargs = {\n","    'phi': norm_affine_phi,\n","    'is_affine_phi': True,\n","}\n","\n","# Keyword arguments for the SGD method wrapped as a dictionary, opened when calling tune_regularization\n","# Feel free to add to this.\n","sgd_kwargs = {\n","    'batch_size': 16,\n","    'n_epochs': 20,\n","    'eta': 0.1,\n","    'verbose': False, # Disable printing INSIDE SGD\n","}\n","\n","clfs = tune_regularization(\n","    xTr, yTr, xVal, yVal,\n","    MultiLogL2Reg,\n","    lmbs,\n","    classifier_kwargs=classifier_kwargs,\n","    sgd_kwargs=sgd_kwargs,\n","    verbose=True\n",")\n","\n","# Select best classifier\n","## Get logged validation errors for the trained classifiers\n","val_errs = [logger.log\n","            for clf in clfs\n","            for logger in clf.sgd_loggers\n","            if logger.name == 'val_err']\n","#### TASK 4 CODE\n","best_idx =\n","best_lmb =\n","best_clf =\n","#### TASK 4 CODE\n","\n","print(best_lmb)\n","\n","# Plot training and validation error w.r.t. lambda\n","#### TASK 4 CODE\n","\n","#### TASK 4 CODE\n","plt.figure()\n","plt.loglog(lmbs, train_errs, marker='.', label='train err')\n","plt.loglog(lmbs, val_errs, marker='.', label='val err')\n","plt.ylabel('empirical err')\n","plt.xlabel('$\\\\lambda$')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ee149jfr1ZzF"},"source":["### [Task 5] Finding the best regularization hyperparameter for L1\n","\n","In this task, you will use `tune_regularization` to find the best $\\lambda$ for L1-regularized logistic classifier. As above, also plot the training and validation error vs $\\lambda$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyKX_swi1ZzG"},"outputs":[],"source":["#### TASK 5 CODE\n","\n","clfs = tune_regularization(\n","    xTr, yTr, xVal, yVal,\n","    MultiLogL1Reg,\n","    lmbs,\n","    classifier_kwargs=classifier_kwargs,\n","    sgd_kwargs=sgd_kwargs,\n","    verbose=True\n",")\n","\n","# Select best classifier\n","## Get logged validation errors for the trained classifiers\n","val_errs =\n","\n","best_idx =\n","best_lmb =\n","best_clf =\n","\n","print(best_lmb)\n","\n","# Plot training and validation error w.r.t. lambda\n","train_errs =\n","plt.figure()\n","\n","\n","\n","plt.ylabel('empirical err')\n","plt.xlabel('$\\\\lambda$')\n","plt.legend()\n","plt.show()\n","#### TASK 5 CODE"]},{"cell_type":"markdown","source":["### [Task 6] Feature Normalization\n","\n","Previously, we mentioned that applying normalization to features helps improve the performance of SGD optimization.\n","In this task, we would like to verify this idea by comparing the performance of classifiers trained with and without feature normalization.\n","\n","Since feature normalization changes the magnitudes of the input data and affects the magnitudes of optimal model weights accordingly, we implement the \"tune_learning_rate\" method to find the best learning rate and ensure that the models are fully optimized. We also turn off regularization to simplify the experiment.\n","\n","Please finish the missing code below and report the validation performance of classifiers trained with and without feature normalization."],"metadata":{"id":"rYe0ljPvuzYT"}},{"cell_type":"code","source":["def tune_learning_rate(\n","    xTr: np.ndarray, yTr: np.ndarray, xVal: np.ndarray, yVal: np.ndarray,\n","    multi_logi_classifier: MultiLogisticReg,\n","    etas: List[float],\n","    classifier_kwargs: Dict[str, Any] = {},\n","    sgd_kwargs: Dict[str, Any] = {},\n","    verbose: bool = True,\n",") -> List[MultiLogisticReg]:\n","    '''\n","    Returns trained classifiers, instantiated with classifier_kwargs\n","    and trained with sgd_kwargs. Each trained classifier to the\n","    corresponding learning rate eta in etas.\n","\n","    Args:\n","        xTr: Training raw features. shape (mTr, *)\n","        yTr: Training labels. shape (mTr)\n","        xVal: Validation raw features. shape (mVal, *)\n","        yVal: Validation labels. shape (mVal)\n","        etas: A list of learning rate values (etas).\n","        multi_logi_classifier: Class name of the multiclass logistic classifier.\n","        classifier_kwargs: (default {}) Keyword arguments to instantiate\n","            multiclass logistic classifier, supplied as a dictionary.\n","        sgd_kwargs: (default {}) Keyword arguments for SGD training,\n","            supplied as a dictionary.\n","        verbose: (default True) Flag for printing progress.\n","\n","    Returns:\n","        clfs: List of trained classifiers. clfs[i] classifier is trained\n","            with eta=etas[i].\n","    '''\n","    sgd_kwargs['verbose'] = sgd_kwargs.pop('verbose', True) and verbose\n","    clfs = []\n","    for eta in etas:\n","        if verbose:\n","            print(f'eta: {eta:.2e} ::: ', end='')\n","\n","        # Train classifier\n","        clf = multi_logi_classifier(**classifier_kwargs)\n","        clf.fit(xTr, yTr, eta=eta, **sgd_kwargs)\n","\n","        # Log train and val err\n","        yTr_pred = clf.predict(xTr)\n","        train_err = utils.empirical_err(yTr, yTr_pred)\n","        train_err_logger = SGDLogger(\n","            'train_err', None, train_err, can_display=True, per_epoch=False\n","        )\n","        clf.sgd_loggers.append(train_err_logger)\n","\n","        yVal_pred = clf.predict(xVal)\n","        val_err = utils.empirical_err(yVal, yVal_pred)\n","        val_err_logger = SGDLogger(\n","            'val_err', None, val_err, can_display=True, per_epoch=False\n","        )\n","        clf.sgd_loggers.append(val_err_logger)\n","\n","        if verbose:\n","            print(f'train_err: {train_err:5f}, val_err: {val_err:5f}')\n","\n","        clfs.append(clf)\n","\n","    val_errs = [logger.log\n","        for clf in clfs\n","        for logger in clf.sgd_loggers\n","        if logger.name == 'val_err']\n","\n","    train_errs = [logger.log\n","        for clf in clfs\n","        for logger in clf.sgd_loggers\n","        if logger.name == 'train_err']\n","\n","    best_idx = np.argmin(val_errs)\n","    best_eta = etas[best_idx]\n","    best_clf = clfs[best_idx]\n","    best_val_err = val_errs[best_idx]\n","\n","    return best_clf, best_eta, best_val_err"],"metadata":{"id":"7g0o5kNvvNv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Values below are a recommendation for initial exploration -- you should see if you want to refine these\n","etas = np.logspace(-4, 0, 5)\n","\n","classifier_kwargs = {\n","    'phi': norm_affine_phi,\n","    'is_affine_phi': True,\n","}\n","\n","sgd_kwargs = {\n","    'batch_size': 16,\n","    'n_epochs': 20,\n","    'verbose': False,\n","}\n","\n","best_clf, best_eta, best_val_err = tune_learning_rate(\n","    xTr, yTr, xVal, yVal,\n","    MultiLogL2Reg,\n","    etas,\n","    classifier_kwargs=classifier_kwargs,\n","    sgd_kwargs=sgd_kwargs,\n","    verbose=True\n",")\n","\n","print(f\"Best validation error trained with feature normalization: {best_val_err}\")\n","\n","#### TASK 6 CODE\n","def null_phi(x):\n","    return\n","\n","classifier_kwargs = {\n","    'phi': null_phi,\n","    'is_affine_phi': True,\n","}\n","\n","\n","\n","\n","\n","\n","\n","#### TASK 6 CODE\n","\n","print(f\"Best validation error trained without feature normalization: {best_val_err}\")"],"metadata":{"id":"_PBFkAeFuzFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [Task 7] Comparing Different Sampling Strategies\n","\n","In the lecture, we introduced mini-batch gradient descent.\n","In this task, please experiment with the four different sampling strategies for the computation of mini-batch gradients implemented in the `SGD` method.\n","\n","- `cycling`: cycle over the data in the input order.\n","- `randperm`: cycle over a random permutation of data fixed across epochs.\n","- `epoch_shuffle`: cycle over a random permutation of data shuffled randomly every epoch.\n","- `iid`: get every batch by iid sampling of the data.\n","\n","Please plot the training error of the classifiers trained with the four sampling strategies as a function of training epochs.\n","Do you observe significant differences between the different strategies?\n","\n","\n","\n"],"metadata":{"id":"biFXao37B4XD"}},{"cell_type":"code","source":["classifier_kwargs = {\n","    'phi': norm_affine_phi,\n","    'is_affine_phi': True,\n","}\n","\n","sgd_kwargs = {\n","    'batch_size': 16,\n","    'eta': 0.1,\n","    'n_epochs': 20,\n","    'verbose': False,\n","}\n","\n","clfs = []\n","sampling_methods = ['cyclic', 'randperm', 'epoch_shuffle', 'iid']\n","#### TASK 7 CODE\n","def get_train_error(w):\n","    return\n","\n","def get_val_error(w):\n","    return\n","\n","for sampling_method in sampling_methods:\n","\n","\n","\n","\n","\n","\n","train_errs =\n","\n","#### TASK 7 CODE\n","\n","plt.figure()\n","for sampling_method, train_err in zip(sampling_methods, train_errs):\n","    plt.plot(np.arange(sgd_kwargs['n_epochs']), train_err, label=f'{sampling_method}')\n","plt.ylabel('train err')\n","plt.xlabel('epochs')\n","plt.legend(title='sampling method')\n","plt.show()"],"metadata":{"id":"UNF7qLHq9U9q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [Task 8] Optimize Learning Rate for Different Batch Sizes\n","\n","Recall that the formulation of mini-batch gradient descent can be written as:\n","$$\n","w_{t+1} \\leftarrow w_t - \\eta \\frac{1}{b} \\sum_{i \\in B_t} \\nabla_w loss(x_i, y_i, w_t)\n","$$\n","where $B_t$ represents the mini-batch sampled at time step $t$, and $b = |B_t|$ represents the batch size.\n","Consider the cases when we train the model for fixed epochs with different batch sizes $b_1, b_2$, where $b_2 \\gg b_1$.\n","Since we have many fewer batches (and iterations) within one epoch when using a large batch size, it is very possible that the model can underfit if we use a fixed learning rate $\\eta$.\n","Therefore, a simple solution is to scale up the learning rate according to the batch size by:\n","$$\n","\\eta_b = \\eta \\cdot b\n","$$\n","\n","\n","In this task, please experiment with different batch sizes and use the tune_learning_rate method provided in Task 6 to find the optimal learning rate $\\eta_b^*$ for different batch sizes.\n","You should find that the optimal learning rates roughly follow the relation $\\frac{\\eta_{b_1}^*}{b_1} \\approx \\frac{\\eta_{b_2}^*}{b_2}$.\n"],"metadata":{"id":"zKqrb0BQU12i"}},{"cell_type":"code","source":["classifier_kwargs = {\n","    'phi': norm_affine_phi,\n","    'is_affine_phi': True,\n","}\n","\n","sgd_kwargs = {\n","    'n_epochs': 10,\n","    'verbose': False,\n","}\n","\n","batch_sizes = [4, 16, 64, 256]\n","best_etas = []  # List to store best eta for each batch size\n","\n","for batch_size in batch_sizes:\n","    #### TASK 8 CODE\n","    etas = np.logspace(-4, 0, 8)\n","\n","\n","\n","\n","    # Store best_eta\n","    #### TASK 8 CODE\n","    print(f\"Batch size = {batch_size}, Best learning rate = {best_eta}\")"],"metadata":{"id":"2ChxgRbcU1Aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [Task 9] Improving Training Efficiency by Increasing Batch Sizes\n","\n","The benefits of gradient descent with large batch sizes include (1) more accurate estimation of gradients and (2) improved efficiency via parallel computation.\n","In this task, we aim to compare the training efficiency of models trained with different batch sizes.\n","\n","In the first part, we use a fixed learning rate across different batch sizes.\n","This setup, however, is not ideal, as we just discussed in Task 8.\n"],"metadata":{"id":"zUroaO6OfVgz"}},{"cell_type":"code","source":["# With fixed learning rates\n","classifier_kwargs = {\n","    'phi': norm_affine_phi,\n","    'is_affine_phi': True,\n","}\n","\n","sgd_kwargs = {\n","    'eta': 0.1,\n","    'n_epochs': 20,\n","    'verbose': False,\n","}\n","\n","import time\n","def log_time(w):\n","    return time.time()\n","\n","clfs = []\n","batch_sizes = [4, 16, 64, 256, 512, 1024]\n","for batch_size in batch_sizes:\n","    sgd_kwargs['batch_size'] = batch_size\n","    clf = MultiLogL2Reg(**classifier_kwargs)\n","    sgd_kwargs['loggers'] = [\n","        SGDLogger('time', log_time, None, can_display=True, per_epoch=True),\n","    ]\n","\n","    clf.fit(xTr, yTr, **sgd_kwargs)\n","    clfs.append(clf)\n","\n","train_objs = [logger.log\n","            for clf in clfs\n","            for logger in clf.sgd_loggers\n","            if logger.name == 'train_obj']"],"metadata":{"id":"OpMJpwKheKeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","To analyze the time consumption of training, we can use $batch\\_size \\times \\#\\ total\\_batches$ as an indicator of the running time for serial computation and use $\\#\\ total\\_batches$ to show the running time for parallel computation.\n","Practically, the execution of a Python program falls somewhere between the fully-serial and fully-parallel cases, so we encourage you to also compare the actual time consumption of your Python program.\n","\n","Please plot the training losses with different batch sizes as a function of (1) $batch\\_size \\times \\#\\ total\\_batches$, (2) $\\#\\ total\\_batches$, and (3) execution time. In the figures, you should obviously see that the models with large batch sizes are still underfitting, as we argued in Task 8."],"metadata":{"id":"6p-B8e2p93OV"}},{"cell_type":"code","source":["plt.figure()\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('parallel runtime (t)')\n","plt.legend()\n","plt.show()\n","\n","### Plotting parallel runtime where x axis stops for all batch sizes at the same value ###\n","plt.figure()\n","max_batch_size = max(batch_sizes)\n","max_x_stop = (sgd_kwargs['n_epochs']) * (len(X) // max_batch_size)\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('parallel runtime (t)')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('serial runtime  (t * b)')\n","plt.legend()\n","plt.show()\n","\n","times = [logger.log\n","         for clf in clfs\n","         for logger in clf.sgd_loggers\n","         if logger.name == 'time']\n","plt.figure()\n","for batch_size, train_obj, time in zip(batch_sizes, train_objs, times):\n","    plt.plot(np.array(time)-time[0], train_obj, label=f'batch size {batch_size}')\n","plt.ylabel('train loss')\n","plt.xlabel('python runtime')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"-vaR8AG6f2FL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the second part, we'll follow the strategy outlined in Task 8 to adjust learning rates according to batch sizes.\n","Repeat what we did in the first part.\n","In the three plots, we should be able to observe that using a large batch size can achieve similar performance with much less time in the case of parallel computing.\n","\n","\n","While you should actually be using the code from Task 8 to tune the step-size for each batch size, this may be time consuming. We will use the observation that the optimal learing rates satisify the relation $\\frac{\\eta_{b_1}^*}{b_1} \\approx \\frac{\\eta_{b_2}^*}{b_2}$.\n","\n","So for simplicity, just use $\\eta_{b}^*=0.025 \\times b$ and see what you observe.\n"],"metadata":{"id":"OIvkReWx_vK0"}},{"cell_type":"code","source":["import time\n","sgd_kwargs = {\n","    'n_epochs': 20,\n","    'verbose': False,\n","}\n","\n","clfs = []\n","#### TASK 9 CODE\n","# Train the models with adjusted learning rates\n","for batch_size in batch_sizes:\n","    sgd_kwargs['batch_size'] =\n","    sgd_kwargs['eta'] =\n","\n","\n","\n","\n","#### TASK 9 CODE\n","\n","train_objs = [logger.log\n","            for clf in clfs\n","            for logger in clf.sgd_loggers\n","            if logger.name == 'train_obj']"],"metadata":{"id":"clFLbKokf2fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('parallel runtime (t)')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('serial runtime  (t * b)')\n","plt.legend()\n","plt.show()\n","\n","times = [logger.log\n","         for clf in clfs\n","         for logger in clf.sgd_loggers\n","         if logger.name == 'time']\n","plt.figure()\n","for batch_size, train_obj, time in zip(batch_sizes, train_objs, times):\n","    plt.plot(np.array(time)-time[0], train_obj, label=f'batch size {batch_size}')\n","plt.ylabel('train loss')\n","plt.xlabel('python runtime')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"IjRPqRxQf28Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **What do you observe when tuning the the step size for batch sizes (we considered so far)? What happens to parallel runtime and serial runtime to achieve a fixed desired loss?**\n","\n","Answer:"],"metadata":{"id":"A9e57k4JUJ3K"}},{"cell_type":"markdown","source":["**We now ask would the same trend continue if we further increase the batch size (even if we tune the step sized appropriately)? Let's explore this!**"],"metadata":{"id":"8OaafwNDVDH_"}},{"cell_type":"code","source":["batch_sizes = [2048, 4096, 8192]"],"metadata":{"id":"jsyRrRwoVZBV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's exactly find the best step sizes for these batch sizes using the code from Task 8."],"metadata":{"id":"Lcwu-_XweMqg"}},{"cell_type":"code","source":["classifier_kwargs = {\n","    'phi': norm_affine_phi,\n","    'is_affine_phi': True,\n","}\n","\n","sgd_kwargs = {\n","    'n_epochs': 100,\n","    'verbose': False,\n","}\n","best_etas = []  # List to store best eta for each batch size\n","\n","for batch_size in batch_sizes:\n","    #### TASK 9 CODE\n","    etas = np.logspace(-1, 1, 3)\n","    ##### Copy your code from Task 8 to tune learning rate in this range\n","\n","\n","    # Store best_eta\n","    #### TASK 9 CODE\n","    print(f\"Batch size = {batch_size}, Best learning rate = {best_eta}\")"],"metadata":{"id":"CTIw06kBIBJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","sgd_kwargs = {\n","    'n_epochs': 100,\n","    'verbose': False,\n","}\n","\n","clfs = []\n","#### TASK 9 CODE\n","# Train the models with adjusted learning rates\n","for (batch_size,best_eta) in zip(batch_sizes,best_etas):\n","    sgd_kwargs['batch_size'] =\n","    sgd_kwargs['eta'] =\n","\n","\n","#### TASK 9 CODE\n","\n","train_objs = [logger.log\n","            for clf in clfs\n","            for logger in clf.sgd_loggers\n","            if logger.name == 'train_obj']"],"metadata":{"id":"qfccVpFnCsMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('parallel runtime (t)')\n","plt.legend()\n","plt.show()\n","\n","\n","plt.figure()\n","for batch_size, train_obj in zip(batch_sizes, train_objs):\n","    #### TASK 9 CODE\n","\n","    #### TASK 9 CODE\n","plt.ylabel('train loss')\n","plt.xlabel('serial runtime  (t * b)')\n","plt.legend()\n","plt.show()\n","\n","times = [logger.log\n","         for clf in clfs\n","         for logger in clf.sgd_loggers\n","         if logger.name == 'time']\n","plt.figure()\n","for batch_size, train_obj, time in zip(batch_sizes, train_objs, times):\n","    plt.plot(np.array(time)-time[0], train_obj, label=f'batch size {batch_size}')\n","plt.ylabel('train loss')\n","plt.xlabel('python runtime')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"uOThcyCZC_tz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What do you observe? What happened to the trend when we further increased the batch size?**\n","\n","Answer: As we increas the batch size further, the parallel runtime no longer improved, but the serial runtime increased linearly."],"metadata":{"id":"xlurTizEW45T"}},{"cell_type":"markdown","metadata":{"id":"r9iliDAE1ZzG"},"source":["### [Task 10] Final classifier\n","\n","As the last task, train a classifier with hyperparameters of your choice.\n","Plot the training/validation loss & error as a function of training iterations, and report the final empirical errors on training and validation dataset splits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30d05EOA1ZzG"},"outputs":[],"source":["#### TASK 10 CODE\n","def get_val_obj(w):\n","    return np.mean(clf.loss_func(norm_affine_phi(xVal) @ w, yVal))\n","\n","\n","\n","\n","#### TASK 10 CODE\n","\n","train_err = utils.empirical_err(yTr, best_clf.predict(xTr))\n","val_err = utils.empirical_err(yVal, best_clf.predict(xVal))\n","\n","\n","print(f'Train err: {train_err:0.5f}')\n","print(f'Validation err: {val_err:0.5f}')"]},{"cell_type":"markdown","source":["### [Task 11] Test Result and Kaggle\n","\n","We are now ready to use our shipped predictor on the test examples `xTe`! You are required to submit predictions on the [Kaggle competition ](https://www.kaggle.com/t/bd133eebbc074c7e9e93e7e965676d3d)as a csv file, using the provided code below. And then you must report your accuracy score from Kaggle, which should match with the benchmark! This part is mendatory."],"metadata":{"id":"XaNXROZQQ4LL"}},{"cell_type":"code","source":["#### Task 11 Code ####\n","test_predictions = best_clf.predict(xTe)"],"metadata":{"id":"HcZYLJUNGdKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"My predictions: {test_predictions}\") ### this should be a list with entries {0,..,.9} ###\n","print(f\"It if of length {len(test_predictions)}.\") ### this should of size X_test = 10000 ###"],"metadata":{"id":"wIeieB4mGljt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's export it as a CSV file and make a Kaggle submission."],"metadata":{"id":"EbnVrkjtGx3k"}},{"cell_type":"code","source":["with open(\"y_predicted.csv\", \"w\") as f:\n","    f.write(\"ID,label\\n\")  # Write header\n","    for i, label in enumerate(test_predictions, start=1):\n","        f.write(f\"{i},{label}\\n\")  # Write ID and label"],"metadata":{"id":"Hh70mTjUG3ms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<span style=\"color: blue\">\n","\n","Answer:\n","\n","\n","My test Accuracy is =\n","</span>"],"metadata":{"id":"rWkkL2kOVg_A"}},{"cell_type":"markdown","source":["### [Task 12] [Challenge] SGD with Different Feature Maps\n","\n","As a challenge, we encourage you to implement SGD with potentially different reature maps, than the naive one we used above. You may also implement the kernalized SGD that we saw in Problem 1 Theory and try out different kernels. Please reuse the code above to generate test prediction on `xTe`, convert into a csv file, and submit on Kaggle to compete with fellow students! And, more importantly, also have fun along the way!"],"metadata":{"id":"9y-0tuVZTKRq"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}