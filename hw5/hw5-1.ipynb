{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "daf09323",
      "metadata": {
        "id": "daf09323"
      },
      "source": [
        "# Homework 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ff58c9",
      "metadata": {
        "id": "14ff58c9"
      },
      "source": [
        "In this notebook, you will explore kernel ridge regression and kernel SVM. We first present kernel ridge regression on a housing dataset to showcase the ideas in Question 3 on the theoretical portion of the homework. Next, we start our exploration into kernel SVM with a two-dimensional example on the spiral data and then build a simple but powerful sentiment classifier on tweets to airlines, a topic we may have more sympathy for as inclement weather hits us here in Chicago..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7785d9",
      "metadata": {
        "id": "0e7785d9"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vm2ZbHdk8MNU",
      "metadata": {
        "id": "vm2ZbHdk8MNU"
      },
      "outputs": [],
      "source": [
        "!gdown 1aZdzUafp91XYlv1xaVj7qpBXfVx8_Pvc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6b4192",
      "metadata": {
        "id": "ec6b4192",
        "outputId": "e29c6bd0-b321-464a-af86-e5f4c5d508c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7197a8",
      "metadata": {
        "id": "4d7197a8"
      },
      "source": [
        "## Kernel Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411e03d8",
      "metadata": {
        "id": "411e03d8"
      },
      "source": [
        "In the Question 3 of the theoretical homework, we studied kernel ridge regression. In this part, we will address the practical considerations for solving the kernel ridge regression problem.\n",
        "\n",
        "We consider the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset), which consists of 8 numeric features, including house age, number of bedrooms, and location, and has target median house value. It contains over 20 thousand samples. We will use 3000 for now, but feel free to use more to see what happens! First, let us load it from `sklearn`'s set of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b9358488",
      "metadata": {
        "id": "b9358488"
      },
      "outputs": [],
      "source": [
        "cal  = sklearn.datasets.fetch_california_housing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3612113",
      "metadata": {
        "id": "e3612113"
      },
      "outputs": [],
      "source": [
        "from utils import TrainAndTestData\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "\n",
        "m = 3000\n",
        "perm = np.random.permutation(len(cal.target))\n",
        "train_i = perm[:m]\n",
        "test_i = perm[m:]\n",
        "train_X = cal.data[train_i,:]\n",
        "train_y = cal.target[train_i]\n",
        "test_X = cal.data[test_i,:]\n",
        "test_y = cal.target[test_i]\n",
        "\n",
        "housing_data = TrainAndTestData(train_X, train_y, test_X, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3ebbce",
      "metadata": {
        "id": "3d3ebbce"
      },
      "source": [
        "Let use a Gaussian (RBF) kernel. Recall that the definition of this kernel is:\n",
        "\n",
        "$$\n",
        "    K_{RBF}(x_i, x_j) = \\exp \\left( -\\beta \\left(  \\langle x_i, x_i \\rangle + \\langle x_j, x_j \\rangle - 2\\langle x_i, x_j \\rangle  \\right)    \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0629ab22",
      "metadata": {
        "id": "0629ab22"
      },
      "outputs": [],
      "source": [
        "def RBF_kernel(beta = 1):\n",
        "    def RBF_kernel_beta(x1,x2):\n",
        "        return np.exp(- beta*(np.sum(x1*x1, 1)[:,np.newaxis] + np.sum(x2*x2, 1)-2*x1@x2.T ))\n",
        "    return RBF_kernel_beta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d4029c",
      "metadata": {
        "id": "70d4029c"
      },
      "source": [
        "*A quick note on closure: the above kernel construction function employs a Python concept known as `closure` which provides us the following functionality. Every kernel function should have the same signature: given two data points as input, output a real number. However, the function may be dependent on some parameter that we cannot hard code. Thus, the outer function constructs a kernel function with the desired signature while fixing a value for the parameter. This becomes very useful when we may need to pass around a kernel function but always for the same parameter value*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10908c5",
      "metadata": {
        "id": "e10908c5"
      },
      "source": [
        "To use this function, we first pick some value of `beta` and instantiate: `RBF_kernel(beta=myvalue)`. This is itself a function and can now take in matrices `x1, x2`. That is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb8a42f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deb8a42f",
        "outputId": "b6825fde-3ab3-4782-d73c-3a37722fdf94"
      },
      "outputs": [],
      "source": [
        "RBF_kernel(beta = 1)(housing_data.X_train[:10], housing_data.X_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411cdf0f",
      "metadata": {
        "id": "411cdf0f"
      },
      "source": [
        "### [Task 1] Looking for a great $\\beta$ value\n",
        "Look at the values of the kernel with $\\beta=1$. What pattern do you observe about the values? Generally, what is the largest value that a Gaussian kernel can achieve? What is the smallest value that a Gaussian kernel can achieve? How does the kernel with $\\beta=1$ compare to the upper bound and the lower bound? You should notice this is not a good choice of $\\beta$. How can you improve this? Find a good setting for $\\beta$ by considering these questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec788ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fec788ec",
        "outputId": "f5f9819a-8c30-4c5c-af4b-d1125b67a8b9"
      },
      "outputs": [],
      "source": [
        "#### TASK 1 CODE\n",
        "beta=\n",
        "RBF_kernel(beta = beta)(housing_data.X_train[:10], housing_data.X_train[:10])\n",
        "#### TASK 1 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b660b84d",
      "metadata": {
        "id": "b660b84d"
      },
      "source": [
        "Armed with a good kernel to represent our features well, we move on to the learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aR6PPNVmuz6",
      "metadata": {
        "id": "4aR6PPNVmuz6"
      },
      "source": [
        "### [Task 2] Implementing Kernel Ridge Regression\n",
        "Now, let us implement the kernel ridge regression solution `train_kernel_ridge`. In 2(b), you formulated the solution to the kernel ridge regression as computing the least squares solution to some expression. `predict_kernel_ridge` will compute the prediction given the kernel, the validation points $x$, the predictor $\\alpha$, and the training points. Finish the functions below using proper numeric python syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a13baf8e",
      "metadata": {
        "id": "a13baf8e"
      },
      "outputs": [],
      "source": [
        "def train_kernel_ridge(kernel, lmbd, x, y):\n",
        "    from numpy.linalg import lstsq\n",
        "    K = kernel(x,x)\n",
        "    #### TASK 2 CODE\n",
        "    least_squares_soln = \n",
        "    #### TASK 2 CODE\n",
        "    return least_squares_soln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "16429bf0",
      "metadata": {
        "id": "16429bf0"
      },
      "outputs": [],
      "source": [
        "def predict_kernel_ridge(kernel, x, alpha, train_x):\n",
        "    #### TASK 2 CODE\n",
        "    return \n",
        "    #### TASK 2 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710a9661",
      "metadata": {
        "id": "710a9661"
      },
      "source": [
        "Given this, load and process the data into the Gram matrix, compute the KRR solution for this data for some fixed regularization parameter $\\lambda$, and predict the answers on a validation set.\n",
        "\n",
        "**Check yourself**: How long does the `train_kernel_ridge` function take to run? Are you inverting a matrix?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a55207e",
      "metadata": {
        "id": "0a55207e"
      },
      "source": [
        "Great. You now are able to compute the KRR solution for data. Let's compute some baselines to know what we're trying to beat. Implement the following two (extremely simple) predictors. We will compute the mean squared error of them later.\n",
        "* **null predictor**: output 0 for every data point\n",
        "* **mean predictor**: output the mean of the training `y` for every data point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87a6076a",
      "metadata": {
        "id": "87a6076a"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(pred, y):\n",
        "    return np.mean((pred-y)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5519a8de",
      "metadata": {
        "id": "5519a8de"
      },
      "outputs": [],
      "source": [
        "#### TASK 2 CODE\n",
        "## BASELINES\n",
        "def null_predictor(x):\n",
        "    return \n",
        "\n",
        "def mean_predictor(train_y, x):\n",
        "    return \n",
        "\n",
        "#### TASK 2 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aa4602",
      "metadata": {
        "id": "d3aa4602"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 1]</h4>\n",
        "\n",
        "For the code block below, use cross-validation or validation to choose $\\lambda$. Make sure you beat the baselines. What values of $\\beta, \\lambda$ give good performance? <br>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "Answer: \n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc77cb76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc77cb76",
        "outputId": "7f8a45de-df03-4952-8e82-50a3570123a5"
      },
      "outputs": [],
      "source": [
        "#### TASK 2 CODE\n",
        "\n",
        "#### TASK 2 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a1d719",
      "metadata": {
        "id": "25a1d719"
      },
      "source": [
        "## 2D kernel SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c74bc9e",
      "metadata": {
        "id": "7c74bc9e"
      },
      "source": [
        "In this part, we see how SVM works on 2d data. We will walk through solving this problem using different kernels. You need to explore different kernels and parameter settings to understand the effect of the hyperparameters. We will use the [implementation of SVM in `scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) which does the following. You can find more details [here](https://scikit-learn.org/1.6/modules/svm.html#svc).\n",
        "\n",
        "Given training vectors $x_i \\in \\mathbb{R}^p$, $i=1,\\dots,n$, in two classes, and a vector $y \\in \\{1, -1\\}^n$, our goal is to find $w \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}$ such that the prediction given by:\n",
        "\n",
        "$$\n",
        "\\text{sign}(w^T \\phi(x) + b)\n",
        "$$\n",
        "\n",
        "is correct for most samples.\n",
        "\n",
        "SVC solves the following primal problem:\n",
        "\n",
        "$$\n",
        "\\min_{w,b,\\zeta} \\quad \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "y_i (w^T \\phi(x_i) + b) \\geq 1 - \\zeta_i,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\zeta_i \\geq 0, \\quad i = 1, \\dots, n\n",
        "$$\n",
        "\n",
        "Intuitively, we're trying to maximize the margin (by minimizing $ \\|w\\|^2 = w^T w $), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value $ y_i (w^T \\phi(x_i) + b) $ would be $ \\geq 1 $ for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance $ \\zeta_i $ from their correct margin boundary.\n",
        "\n",
        "The penalty term $ C $ controls the strength of this penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094a06e9",
      "metadata": {},
      "source": [
        "`svm.SVC` supports built-in **RBF, polynomial, and linear kernels**, which we have covered in class. To help you understand how they work, we implement a **polynomial kernel** below. You can also find the code for the **RBF kernel** at the beginning of the notebook.\n",
        "\n",
        "To verify that everything works correctly, try using both **our defined kernel** and `SVC`'s built-in kernels and compare the results.\n",
        "\n",
        "We have prepared three toy demo datasets, each containing 10 positive samples and 10 negative samples. You are encouraged to run SVM on each demo set and explore different **kernels**, **hyperparameters**, and **regularization settings** to understand how these factors influence the learned classifier.\n",
        "- Spend some time analyzing the impact of these choices, as they will be critical in later sections.\n",
        "- In the next part, you will work with **higher-dimensional data**, where visualization is not as straightforward. This makes it even more important to understand how hyperparameters affect your model now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8a9d9f5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import get_demo\n",
        "\n",
        "def poly_kernel(degree = 2, beta = 1 , a = 1):\n",
        "    def poly_kernel_inst(x1,x2):\n",
        "        return (beta * x1@x2.T + a)**degree\n",
        "    return poly_kernel_inst\n",
        "\n",
        "\n",
        "def train_and_plot_svm(data, C=None, **args):\n",
        "    svm_clf = svm.SVC(**args,C=C)\n",
        "    svm_clf = svm_clf.fit(data.X_train, data.y_train)\n",
        "    utils.plot_decision_boundary_with_svm(svm_clf, data.X_train, data.y_train, data.X_test, data.y_test, C=C)\n",
        "    # utils.plot_decision_boundary_with_svm(svm_clf, data.X_train, data.y_train, None, None, C=C)\n",
        "    # you can also avoid passing C by checking for margin violations based on the response being less then 1\n",
        "    if data.y_test is not None:\n",
        "        data.print_errors(svm_clf)\n",
        "    return svm_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb83da78",
      "metadata": {},
      "outputs": [],
      "source": [
        " # Select demo by choosing 1, 2, or 3\n",
        "demo = get_demo(1)  # Change the number (1, 2, or 3) to load a different demo\n",
        "\n",
        "# Set hyperparameters for the SVM model\n",
        "C = 5       \n",
        "gamma = 1  \n",
        "\n",
        "# Train and plot SVM using a custom RBF kernel (default option)\n",
        "svm_clf = train_and_plot_svm(demo, C=C, kernel=RBF_kernel(beta=gamma))\n",
        "\n",
        "# Alternative: Use the built-in RBF kernel from sklearn's SVC (if needed)\n",
        "# Uncomment the line below to use sklearn's built-in RBF kernel instead of the custom one\n",
        "# svm_clf = train_and_plot_svm(demo, C=C, kernel='rbf', gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffd4c43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select demo by choosing 1, 2, or 3\n",
        "demo = get_demo(1)  # Change the number to load a different demo\n",
        "\n",
        "# Set SVM hyperparameters\n",
        "C = 5\n",
        "degree = 3\n",
        "beta = 2\n",
        "a = 1\n",
        "\n",
        "# Train and plot SVM using the custom polynomial kernel (default)\n",
        "svm_clf = train_and_plot_svm(demo, C=C, kernel=poly_kernel(degree=degree, beta=beta, a=a))\n",
        "\n",
        "# Alternative: Use the built-in polynomial kernel from sklearn's SVC\n",
        "# Uncomment the line below to use sklearn's polynomial kernel instead of the custom one\n",
        "# svm_clf = train_and_plot_svm(demo, C=C, kernel='poly', degree=degree, gamma=beta, coef0=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c382a5",
      "metadata": {},
      "source": [
        "Now we will move from toy demo data to spiral dataset that you are very familiar with. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "mXSAPW9C_to3",
      "metadata": {
        "id": "mXSAPW9C_to3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "LABELS = [-1, 1]\n",
        "SP_THETA_SIGMA = 0.3\n",
        "SP_R_SIGMA = 0.05\n",
        "NOISE_LEVEL = 0.2\n",
        "\n",
        "m = 1000\n",
        "Xsp, ysp = utils.generate_spiral_data(m, noise_level=NOISE_LEVEL, theta_sigma=SP_THETA_SIGMA, r_sigma=SP_R_SIGMA)\n",
        "\n",
        "train_test_ratio = 0.8\n",
        "Xsp_train, ysp_train, Xsp_test, ysp_test = utils.create_split(Xsp, ysp, train_test_ratio)\n",
        "\n",
        "spirals = TrainAndTestData(Xsp_train, ysp_train, Xsp_test, ysp_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c559e2e",
      "metadata": {
        "id": "6c559e2e"
      },
      "source": [
        "Explore how the **RBF kernel** performs on the spiral dataset by varying gamma(beta) and observing its effect on the decision boundary.  \n",
        "- Try **different values of gamma**, including extreme ones, to understand how it influences model behavior.  \n",
        "- You can find more details on **gamma in the RBF kernel** [here](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html#rbf-kernel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972d42ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set hyperparameters for the SVM model\n",
        "C = 5       \n",
        "gamma = 1  \n",
        "\n",
        "# Train and plot SVM using a custom RBF kernel (default option)\n",
        "svm_clf = train_and_plot_svm(spirals, C=C, kernel=RBF_kernel(beta=gamma))\n",
        "\n",
        "# Alternative: Use the built-in RBF kernel from sklearn's SVC (if needed)\n",
        "# Uncomment the line below to use sklearn's built-in RBF kernel instead of the custom one\n",
        "# svm_clf = train_and_plot_svm(spirals, C=C, kernel='rbf', gamma=gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f58189f5",
      "metadata": {
        "id": "f58189f5"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 2]</h4>\n",
        "How does the choice of gamma affect the decision boundary? What happens when gamma is set to very high or very low values?\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "\n",
        "Answer:\n",
        "\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336bde65",
      "metadata": {
        "id": "336bde65"
      },
      "source": [
        "See how polynoimal kernel works on the spiral data. You should try different numbers for degree, scaling factor gamma(beta) and bias term a to see how the decision boundard looks like. The hyperparameters are defined [here](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html#polynomial-kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e27d6d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set SVM hyperparameters\n",
        "C = 5\n",
        "degree = 3\n",
        "gamma = 2\n",
        "a = 1\n",
        "\n",
        "# Train and plot SVM using the custom polynomial kernel (default)\n",
        "svm_clf = train_and_plot_svm(spirals, C=C, kernel=poly_kernel(degree=degree, beta=gamma, a=a))\n",
        "\n",
        "# Alternative: Use the built-in polynomial kernel from sklearn's SVC\n",
        "# Uncomment the line below to use sklearn's polynomial kernel instead of the custom one\n",
        "# svm_clf = train_and_plot_svm(spirals, C=C, kernel='poly', degree=degree, gamma=gamma, coef0=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1139224",
      "metadata": {},
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 3]</h4>\n",
        "How does the choice of degree, scaling factor gamma(beta) and bias term a affect the decision boundary?\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "\n",
        "Answer:\n",
        "\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83d5235",
      "metadata": {
        "id": "b83d5235"
      },
      "source": [
        "### [Task 3] Applying SVM classifiers to the 2D spiral data\n",
        "For the classifier you trained, you need to finish code below to compute training loss, training error, margin violations and the number of support vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce33d559",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce33d559",
        "outputId": "5cc048b8-3c5c-437a-aab7-bf234ea0ff90"
      },
      "outputs": [],
      "source": [
        "#### TASK 3 CODE\n",
        "# training loss: hinge loss of svm_clf\n",
        "train_loss = \n",
        "print(f\"Hinge loss: {train_loss}\")\n",
        "\n",
        "# training error\n",
        "train_err = \n",
        "print(f\"Train error: {train_err}\")\n",
        "\n",
        "# margin violations\n",
        "margin_violations = \n",
        "print(f\"Margin violations: {margin_violations}\")\n",
        "\n",
        "# number of support vectors\n",
        "num_support_vectors = \n",
        "print(f\"Number of support vectors: {num_support_vectors}\")\n",
        "#### TASK 3 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4749feb1",
      "metadata": {
        "id": "4749feb1"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "Answer:\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7272f9eb",
      "metadata": {
        "id": "7272f9eb"
      },
      "source": [
        "## Sentiment Analysis Using Kernel SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3376d040",
      "metadata": {
        "id": "3376d040"
      },
      "source": [
        "Next, we will develop a sentiment classifier using kernel SVM. We will work with a real-world dataset of [tweets to airlines](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/version/2). Datasets that have been scraped from the internet (such as this one) are prone to many issues, whether we use them directly or with some filtering. See if you can think of a few potential issues, and feel free to discuss with TAs at office hours. Despite these issues, the dataset provides value in giving us short statements with strong sentiment that we will build a classifier over.\n",
        "\n",
        "**Data pre-processing** The raw data, which you can access and study at the above link, contains 15 attributes, including `tweet_id` , `airline_sentiment`, `negative_reason`, `airline`, `text.` Of these, we are most interested in `airline_sentiment` and `text`. To that end, we have extracted these for you in the files `cleaned_tweets_train.tsv`. A `tsv` file is a file where the different attributes are separated by tabs. The dataset identifies three different sentiments: `positive`, `neutral`, and `negative`. After extracting just the tweets and the sentiments, we shuffled all the tweets and saved the first 3/4 of them to the training file and the remaining 1/4 of them to the test file.\n",
        "\n",
        "**Data loading** The `load_data` function we provide in `utils.py` allows filtering neutral (i.e., removing them from the data) by setting `filter_neutrals` flag to `True` or including them but counting them as \"not positive\" examples by setting `filter_neutrals` flag to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAZ9_cceEgfk",
      "metadata": {
        "id": "DAZ9_cceEgfk"
      },
      "outputs": [],
      "source": [
        "!mkdir $PWD/data/\n",
        "import gdown\n",
        "gdown.download(f\"https://drive.google.com/uc?id=1acMRrRzHQdni-WVtMZOTwOEQGmcGWvcM\", \"./data/cleaned_tweets_train.tsv\", quiet=False)\n",
        "gdown.download(f\"https://drive.google.com/uc?id=1S_o4sBJs2uWJfEPLsykzsgBM8gYx_gzw\", \"./data/cleaned_tweets_test.tsv\", quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629010c8",
      "metadata": {
        "id": "629010c8"
      },
      "source": [
        "### Kernel Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a5c310",
      "metadata": {
        "id": "31a5c310"
      },
      "source": [
        "Recall that a kernel can be defined as $K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle\\, \\in \\mathbb{R}\\,.$ However, the $x_i$ does not have to be real-valued, or numeric at all. Indeed, in this case, they are strings of length $k$ (tweets, in particular) in $\\mathcal{D}^k$, where $\\mathcal{D}$ is the dictionary of words. Then, we can decompose the kernel as:\n",
        "\n",
        "\n",
        "$$\n",
        "K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\langle \\tilde{\\phi}(v(x_i)), \\tilde{\\phi}(v(x_j))\\rangle\\\n",
        "$$\n",
        "\n",
        "\n",
        "where $\\phi = \\tilde{\\phi} \\circ v\\,,$ $\\tilde{\\phi} \\, : \\mathbb{R}^{d_1} \\mapsto \\mathbb{R}^{d_2}$ and $v \\, : \\mathcal{D}^k \\mapsto \\mathbb{R}^{d_1}\\,.$\n",
        "\n",
        "This decomposition allows us to separate the transformation into two parts. Now, we can choose both independently. Here are some suggestions for each:\n",
        "\n",
        "* For $v(x_i)\\,:$\n",
        "    * **Bag-of-words**: for each word $w$ in the corpus, the corresponding component of the bag-of-words representation of $x_i$ is defined as the number of occurences of $w$ in $x_i$.\n",
        "    * **Bi-gram**: for each pair of words that occur contiguously in the corpus, the corresponding component of the bi-gram representation of $x_i$ is the number of times that the bi-gram (two-word pattern) appears in $x_i$.\n",
        "    * **Subsequence counts**: for each subsequence (of fixed size) in the corpus, the corresponding component of this representation of $x_i$ is the number of times that the subsequence has appeared in the document $x_i.$ (A subsequence allows for skipping characters, whereas a substring is all continguous characters.)\n",
        "    * ...others?\n",
        "\n",
        "\n",
        "\n",
        "* For $\\tilde{\\phi}\\,:$\n",
        "    * Linear\n",
        "    * Polynomial\n",
        "    * Radial basis function\n",
        "    * Weighted cosine similarity\n",
        "    * ...others?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88a1e1a",
      "metadata": {
        "id": "e88a1e1a"
      },
      "source": [
        "**Check your understanding**:\n",
        "* what is the dimension of a bag-of-words representation of a sentence? what about bigram?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b40782c3",
      "metadata": {
        "id": "b40782c3"
      },
      "outputs": [],
      "source": [
        "def BoW_inner(s1,s2):\n",
        "    \"returns inner product between bag-of-word feature vectors of the two input strings\"\n",
        "    from collections import Counter\n",
        "    d1 = Counter(s1.split())\n",
        "    return sum(d1[w] for w in s2.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c077d57",
      "metadata": {
        "id": "0c077d57"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 4]</h4>\n",
        "\n",
        "No coding in this one: Consider the BoW kernel constructed in `BoW_inner`. Suppose there are $D$ words in the corpus, and each sentence (document) has at most $k$ words.\n",
        "* What is the time and space complexity of naively constructing the bag-of-words vector for each sentence and computing their inner product?\n",
        "* What is the time and space complexity of the implementation in the code?\n",
        "* What accounts for the difference? <br>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "\n",
        "Answer:\n",
        "\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d4c5b6",
      "metadata": {
        "id": "87d4c5b6"
      },
      "source": [
        "### [Task 4] Implementing your own feature mapping function for text data\n",
        "\n",
        "Think about your own understanding of sentences. What features do you use to understand them? **Hand design a feature mapping from sentence to numeric values that might help a kernel learn to classify sentiment.** To do this, you may wish to load the training data and inspect what positive and negative samples look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a5cbf7",
      "metadata": {
        "id": "14a5cbf7"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = utils.load_data(os.path.join(os.getcwd(),\"data/cleaned_tweets_train.tsv\"),type=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8126bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d8126bf",
        "outputId": "6d081ac3-57a4-41dd-a3d0-ac907a4574ee"
      },
      "outputs": [],
      "source": [
        "print(\"---- positive samples ----\")\n",
        "for i in range(100):\n",
        "    if y_train[i] == 1:\n",
        "        print(X_train[i])\n",
        "print(\"--------------------------\")\n",
        "print(\"---- negative samples ----\")\n",
        "for i in range(100):\n",
        "    if y_train[i] == -1:\n",
        "        print(X_train[i])\n",
        "print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1d64e391",
      "metadata": {
        "id": "1d64e391"
      },
      "outputs": [],
      "source": [
        "def my_feature_map(x1): ### You should rename this to be more descriptive about the feature mapping that you are using\n",
        "    #### TASK 4 CODE\n",
        "    # This is an open question, so we will not provide a fixed solution, but you\n",
        "    # are encouraged to share your ideas in the course discussion over Canvas!\n",
        "    #### TASK 4 CODE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "382c06e9",
      "metadata": {
        "id": "382c06e9"
      },
      "outputs": [],
      "source": [
        "def my_inner_product(x1, x2):\n",
        "    '''\n",
        "    this function computes the inner product phi(x1)*phi(x2) for phi,\n",
        "        the feature transform defined in the previous cell, i.e., my_feature_map\n",
        "    implementing this as np.dot(my_feature_map(x1), my_feature_map(x2)) is not\n",
        "        super-useful, as the runtime will be at least linear in the dimension of\n",
        "        the feature map. Instead, implement this without ever using my_feature_map.\n",
        "    '''\n",
        "    #### TASK 4 CODE\n",
        "    # This is an open question, so we will not provide a fixed solution, but you\n",
        "    # are encouraged to share your ideas in the course discussion over Canvas!\n",
        "    #### TASK 4 CODE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65b65ac",
      "metadata": {
        "id": "b65b65ac"
      },
      "source": [
        "Next, we compute the Gram matrix from any kernel we have implemented (e.g., the one you just implemented or the bag-of-words example given in function `BoW_inner`. This is useful in computing $\\tilde{\\phi}$ for $\\tilde{\\phi}$ that can be vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3f599ca4",
      "metadata": {
        "id": "3f599ca4"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(K):\n",
        "    def gram_matrix_K(xs_1, xs_2):\n",
        "        return np.array([[K(x1, x2) for x2 in xs_2] for x1 in xs_1])\n",
        "    return gram_matrix_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "772b295f",
      "metadata": {
        "id": "772b295f"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel_gram(inner, beta=1):\n",
        "    \"\"\"Gaussian RBF kernel.\n",
        "\n",
        "    Returns a functoin gram(xs_1,xs_2) that calculate the (cross) gram matrix G[i,j]=K(xs_1[i,xs_2[j]])\n",
        "    where K is the Gaussian RBF on the features phi, specified through the inner product in phi space.\"\"\"\n",
        "    def rbf_kernel_sigma_inner(xs_1,xs_2):\n",
        "        return np.exp(-beta*(np.array([inner(x1, x1) for x1 in xs_1])[:, np.newaxis]\n",
        "                             + np.array([inner(x2, x2) for x2 in xs_2])\n",
        "                             - 2*gram_matrix(inner)(xs_1, xs_2)))\n",
        "    return rbf_kernel_sigma_inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ca4db925",
      "metadata": {
        "id": "ca4db925"
      },
      "outputs": [],
      "source": [
        "def poly_kernel_gram(inner, deg, alpha=1.0):\n",
        "    def poly_kernel_deg_alpha(xs_1, xs_2):\n",
        "        return (alpha + gram_matrix(inner)(xs_1, xs_2))**deg\n",
        "    return poly_kernel_deg_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cac6cd2",
      "metadata": {
        "id": "5cac6cd2"
      },
      "source": [
        "Let us see how to generate the RBF kernel matrix using `BoW_inner` and `my_inner_product` as $\\langle v(x_i), v(x_j)\\rangle$ for some value of the parameter $\\beta\\,.$ Note that `rbf_kernel_gram(inner, beta)` returns a function, and we pass it the datasets as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac763e16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac763e16",
        "outputId": "f4750fe7-bb4f-427f-d462-3a529957cf72"
      },
      "outputs": [],
      "source": [
        "rbf_kernel_gram(BoW_inner,0.2)(X_train[:10],X_train[5:12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNKwtXZRrLOI",
      "metadata": {
        "id": "bNKwtXZRrLOI"
      },
      "outputs": [],
      "source": [
        "rbf_kernel_gram(my_inner_product,0.2)(X_train[:10],X_train[5:12])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0411bae",
      "metadata": {
        "id": "d0411bae"
      },
      "source": [
        "### [Task 5] Optimizing your SVM classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ddad657",
      "metadata": {
        "id": "7ddad657"
      },
      "source": [
        "Now that we know how to extract and kernelize our data, let us train SVM on it. As an example, here is how you might use `sklearn`'s SVM implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6160b3a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6160b3a5",
        "outputId": "48e34385-c125-4712-8ca7-f4143ec1e138"
      },
      "outputs": [],
      "source": [
        "# fit svm with a small subset of data\n",
        "svm_clf = svm.SVC(kernel=poly_kernel_gram(BoW_inner, 2))\n",
        "svm_clf= svm_clf.fit(X_train[:100], y_train[:100])\n",
        "preds = svm_clf.predict(X_train[100:120])\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7588c310",
      "metadata": {},
      "source": [
        "Here, poly_kernel_gram computes the polynomial kernel matrix on the fly. This means that every time `SVC` is called, it must re-compute all pairwise inner products between feature vectors. While this approach is fine for small datasets, it becomes computationally expensive for large-scale problems. \n",
        "### Using a Precomputed Kernel\n",
        "\n",
        "To improve efficiency, we can **precompute the kernel matrix** before passing it to `SVC`. This allows us to reuse the computed values instead of recalculating them multiple times.\n",
        "\n",
        "#### How Precomputed Kernels Work in SVM\n",
        "\n",
        "- Instead of providing raw feature vectors to `SVC`, we **compute and store the Gram matrix** (a matrix of pairwise similarities).\n",
        "- This Gram matrix replaces the need for direct feature input.\n",
        "- The model expects a **square matrix** for training (size $ n \\times n$), where each entry represents the kernel function applied to a pair of training samples.\n",
        "- During testing, we compute a **test vs. train kernel matrix** (size $ m \\times n $) to compare new samples against the training set.\n",
        "\n",
        "Below is the example and you will see it gives the same prediction as above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8815490d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the Gram matrix for training\n",
        "K_train = poly_kernel_gram(BoW_inner, 2) (X_train[:100], X_train[:100])\n",
        "\n",
        "# Train SVM using the precomputed kernel matrix\n",
        "svm_clf = svm.SVC(kernel='precomputed')\n",
        "svm_clf.fit(K_train, y_train[:100])\n",
        "\n",
        "# Compute the Gram matrix for testing \n",
        "K_test = poly_kernel_gram(BoW_inner, 2) (X_train[100:120], X_train[:100]) # Notice: X_test vs X_train\n",
        "\n",
        "# Predict using the precomputed test kernel\n",
        "preds = svm_clf.predict(K_test)\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8214524",
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit svm with entire training data\n",
        "# it can take several minutes to run\n",
        "svm_clf = svm.SVC(kernel=poly_kernel_gram(BoW_inner, 2))\n",
        "svm_clf= svm_clf.fit(X_train, y_train)\n",
        "preds = svm_clf.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd601d98",
      "metadata": {},
      "source": [
        "Note that $C = 1/\\lambda\\,,$ where $\\lambda$ is the regularization parameter we discussed in class. Use a validation set to evaluate the performance of the classifiers you train."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ecc25d1",
      "metadata": {
        "id": "3ecc25d1"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 5]</h4>\n",
        "\n",
        "\n",
        "<!-- <h4 style=\"font-weight: bold\">---------------------</h4> -->\n",
        "\n",
        "We give you a set of questions below to explore and some direction regarding how to explore them. As your \"answer\" for this section, submit a write-up in the notebook with 2-3 plots about the answers to these questions. Also submit your code as applicable. You can finish them with a small subset of training data but you are encouraged to try the entire training dataset with the precomputed kernel trick. \n",
        "\n",
        "* For different kernels, train SVM with different $\\lambda$ spanning a good range. Use cross validation to determine a good value of $\\lambda$. What are the resulted (1) 0-1, Hinge training loss? (2) Margin loss? (3) Test error? (4) Support Vectors? If using a precomputed kernel, cross-validation requires special handling. Since the Gram matrix depends on the training set, you need to extract the corresponding submatrices for each training split in cross-validation. The training submatrix should only include training samples, and the validation submatrix should compare validation samples against the training set.\n",
        "\n",
        "* Identify examples where the classifier fails for different kernels. Speculate on what the various kernels might be more suited to.\n",
        "\n",
        "* You implemented your own kernel: how did that do? Did the performance match what you were expecting? If not, what factors might have influenced that?\n",
        "\n",
        "* Consider the various attributes of a machine learning algorithm we may be interested in practically: generalization, runtime, memory usage, ease of implementation, understandability. How does kernel SVM for the kernels you tried perform on each of these attributes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20b78d8",
      "metadata": {},
      "source": [
        "<span style=\"color: blue\">\n",
        "Answer: \n",
        "</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6da9548",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "After evaluating different classifiers you developed based on various design choices, evaluate the performance of the one you've chosen to \"ship\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f842a7",
      "metadata": {
        "id": "63f842a7"
      },
      "outputs": [],
      "source": [
        "def my_final_predictor(x_data_to_predict_on): # you might change the input if you use precomputed kernels\n",
        "    #### TASK 5 CODE\n",
        "    my_predictions =      # an array of +1/-1 the same length as x_data_to_predict\n",
        "    #### TASK 5 CODE\n",
        "    return my_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lQWJQobhsO7r",
      "metadata": {
        "id": "lQWJQobhsO7r"
      },
      "source": [
        "You are now ready to ship your predictor!!  Hurray!!  \n",
        "Let's use it, and see how well it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a33754",
      "metadata": {
        "id": "51a33754"
      },
      "outputs": [],
      "source": [
        "X_test = utils.load_data(os.path.join(os.getcwd(), \"data/cleaned_tweets_test.tsv\"),type=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2c571d71",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_predictions = my_final_predictor(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff9d80f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"My predictions: {test_predictions}\") ### this should be a list of +1/-1 ###\n",
        "print(f\"It if of length {len(test_predictions)}.\") ### this should of size X_test = 2192 ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adcb0f13",
      "metadata": {},
      "source": [
        "Yay, It did not crash!! Now let us convert the list into a csv file. You can export the tsv file and submit it on [Kaggle](https://www.kaggle.com/t/ed3bead259169073ec68e3e9fbf6b247), to see yourself on the leaderboard!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87882a25",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"y_predicted.csv\", \"w\") as f:\n",
        "    f.write(\"ID,label\\n\")  # Write header\n",
        "    for i, label in enumerate(test_predictions, start=1):\n",
        "        f.write(f\"{i},{label}\\n\")  # Write ID and label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "optimization",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
